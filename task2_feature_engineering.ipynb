{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:08:26.328276Z",
     "start_time": "2021-02-02T10:08:24.777830Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\"\"\"\n",
    "Python中，主要依靠gc（garbage collector）模块的引用计数技术来进行垃圾回收。\n",
    "所谓引用计数，就是考虑到Python中变量的本质不是内存中一块存储数据的区域，而是对一块内存数据区域的引用。\n",
    "所以python可以给所有的对象（内存中的区域）维护一个引用计数的属性，\n",
    "在一个引用被创建或复制的时候，让python把相关对象的引用计数+1；相反当引用被销毁的时候就把相关对象的引用计数-1。\n",
    "当对象的引用计数减到0时，自然就可以认为整个python中不会再有变量引用这个对象，\n",
    "所以就可以把这个对象所占据的内存空间释放出来了。\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T11:06:05.168210Z",
     "start_time": "2021-01-05T11:06:05.164045Z"
    }
   },
   "source": [
    "# 配置路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:08:27.783836Z",
     "start_time": "2021-02-02T10:08:27.775502Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "tmp_res_path = 'tmp_results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T11:06:15.361414Z",
     "start_time": "2021-01-05T11:06:15.357543Z"
    }
   },
   "source": [
    "# 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:08:28.927837Z",
     "start_time": "2021-02-02T10:08:28.923255Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据函数\n",
    "def read_data(file_name):\n",
    "\n",
    "    df = pd.read_csv(data_path + file_name)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T11:09:29.013729Z",
     "start_time": "2021-01-05T11:09:29.007465Z"
    }
   },
   "source": [
    "# 对数据进行内存压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:08:30.165625Z",
     "start_time": "2021-02-02T10:08:30.149301Z"
    }
   },
   "outputs": [],
   "source": [
    "# 节约内存的一个标配函数\n",
    "def reduce_memory(df, verbose=True):\n",
    "    \n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'\n",
    "          .format(end_mem, 100*(start_mem-end_mem)/start_mem, (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:10:10.128011Z",
     "start_time": "2021-02-02T10:08:30.986348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to  3.73 Mb (53.1% reduction),time spend:0.00 min\n",
      "-- Mem. usage decreased to  5.49 Mb (31.2% reduction),time spend:0.00 min\n",
      "-- Mem. usage decreased to  6.47 Mb (50.0% reduction),time spend:0.00 min\n",
      "-- Mem. usage decreased to 981.69 Mb (60.9% reduction),time spend:0.13 min\n"
     ]
    }
   ],
   "source": [
    "num_of_rows = None\n",
    "\"\"\"\n",
    "# train & test\n",
    "test_data_path = data_path + 'test_format1.csv'\n",
    "train_data_path = data_path + 'train_format1.csv'\n",
    "\n",
    "# user_info & user_log\n",
    "user_info_path = data_path + 'user_info_format1.csv'\n",
    "user_log_path = data_path + 'user_log_format1.csv'\n",
    "\"\"\"\n",
    "# STEP1. read data from csv\n",
    "# train & test\n",
    "test_data = read_data(file_name='test_format1.csv')\n",
    "train_data = read_data(file_name='train_format1.csv')\n",
    "\n",
    "# user_info & user_log\n",
    "user_info = read_data(file_name='user_info_format1.csv')\n",
    "user_log = read_data(file_name='user_log_format1.csv')\n",
    "\n",
    "# STEP2. reduce memory\n",
    "train_data = reduce_memory(train_data)\n",
    "test_data = reduce_memory(test_data)\n",
    "user_info = reduce_memory(user_info)\n",
    "user_log = reduce_memory(user_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T02:46:37.209611Z",
     "start_time": "2021-01-06T02:46:13.407Z"
    }
   },
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:44:49.193006Z",
     "start_time": "2021-01-11T10:44:49.131328Z"
    }
   },
   "outputs": [],
   "source": [
    "# 合并训练&测试数据集\n",
    "df_all_data = train_data.append(test_data)\n",
    "df_all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:44:49.432876Z",
     "start_time": "2021-01-11T10:44:49.198779Z"
    }
   },
   "outputs": [],
   "source": [
    "# 合并用户信息&全量数据集\n",
    "df_all_data = pd.merge(df_all_data, user_info, on='user_id', how='left')\n",
    "df_all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:44:49.888688Z",
     "start_time": "2021-01-11T10:44:49.440877Z"
    }
   },
   "outputs": [],
   "source": [
    "# 清除垃圾\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:45:06.202557Z",
     "start_time": "2021-01-11T10:44:49.891630Z"
    }
   },
   "outputs": [],
   "source": [
    "# 按照时间排序\n",
    "user_log = user_log.sort_values(by=['user_id', 'time_stamp'])\n",
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:47:45.389394Z",
     "start_time": "2021-01-11T10:45:06.205531Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对于每个用户合并字段\n",
    "# cat_id, seller_id, brand_id, time_stamp, action_type\n",
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "agg_dict = {\n",
    "    'item_id': list_join_func,\n",
    "    'cat_id': list_join_func,\n",
    "    'seller_id': list_join_func,\n",
    "    'brand_id': list_join_func, \n",
    "    'time_stamp': list_join_func, \n",
    "    'action_type': list_join_func\n",
    "}\n",
    "\n",
    "rename_dict = {\n",
    "    'item_id': 'item_path',\n",
    "    'cat_id': 'cat_path',\n",
    "    'seller_id': 'seller_path',\n",
    "    'brand_id': 'brand_path', \n",
    "    'time_stamp': 'time_stamp_path', \n",
    "    'action_type': 'action_type_path'\n",
    "}\n",
    "\n",
    "def merga_list(df_id, join_col, df_data, agg_dict, rename_dict):\n",
    "    \n",
    "    df_data = df_data.groupby(join_col).agg(agg_dict).reset_index().rename(columns=rename_dict)\n",
    "    df_id = pd.merge(df_id, df_data, on=join_col, how='left')\n",
    "    \n",
    "    return df_id\n",
    "    \n",
    "\n",
    "df_all_data = merga_list(df_all_data, 'user_id', user_log, agg_dict, rename_dict)\n",
    "df_all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:47:45.764083Z",
     "start_time": "2021-01-11T10:47:45.401582Z"
    }
   },
   "outputs": [],
   "source": [
    "# 删除数据并回收内存\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T01:01:29.315736Z",
     "start_time": "2021-01-08T01:01:29.305542Z"
    }
   },
   "source": [
    "# 定义特征统计函数\n",
    "## 定义统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:47:45.791711Z",
     "start_time": "2021-01-11T10:47:45.767884Z"
    }
   },
   "outputs": [],
   "source": [
    "# total count of data\n",
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# number of unique element of data\n",
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "# maximum of data\n",
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        -1\n",
    "        \n",
    "# minimum of data\n",
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        -1\n",
    "        \n",
    "# standard deviation of data\n",
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        -1\n",
    "        \n",
    "# the top k element of data\n",
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        -1\n",
    "        \n",
    "# total count of the top k element of data\n",
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:47:45.806365Z",
     "start_time": "2021-01-11T10:47:45.795324Z"
    }
   },
   "outputs": [],
   "source": [
    "Counter('1 2 8 10 13 20 20'.split(' ')).most_common(3)[3-1][0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T10:47:45.825253Z",
     "start_time": "2021-01-11T10:47:45.809665Z"
    }
   },
   "outputs": [],
   "source": [
    "def user_cnt(df_data, single_col, name):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df_data: DataFrame\n",
    "    @param single_col: the col name to apply stats\n",
    "    @param name: new col name to create\n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df_data: DataFrame\n",
    "    @param single_col: the col name to apply stats\n",
    "    @param name: new col name to create\n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_max(df_data, single_col, name):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df_data: DataFrame\n",
    "    @param single_col: the col name to apply stats\n",
    "    @param name: new col name to create\n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df_data: DataFrame\n",
    "    @param single_col: the col name to apply stats\n",
    "    @param name: new col name to create\n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_std(df_data, single_col, name):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df_data: DataFrame\n",
    "    @param single_col: the col name to apply stats\n",
    "    @param name: new col name to create\n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df_data: DataFrame\n",
    "    @param single_col: the col name to apply stats\n",
    "    @param name: new col name to create\n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df_data: DataFrame\n",
    "    @param single_col: the col name to apply stats\n",
    "    @param name: new col name to create\n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    \n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T03:28:44.720661Z",
     "start_time": "2021-01-09T03:28:44.698513Z"
    }
   },
   "source": [
    "## 提取统计特征\n",
    "### 特征统计\n",
    "用户复购行为，考虑用户自身因素和商家因素，一般来说是商家提供的产品和服务能较好地满足用户需求；\n",
    "#### 店铺特征\n",
    "统计基于商户的特征主要目的是分析商户在当前市场的受欢迎程度及商户自身对忠实用户的吸引力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:55:36.386430Z",
     "start_time": "2021-01-19T04:55:35.878674Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "店铺的基本统计特征: 店铺/商品/品牌\n",
    "\"\"\"\n",
    "# 选取数据测试/全量数据开关\n",
    "df_all_data_test = df_all_data.head(1)  # 少量数据测试代码逻辑\n",
    "# df_all_data_test = df_all_data  # 全量数据构建特征\n",
    "print('len', len(df_all_data_test))\n",
    "print(df_all_data_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:27.050387Z",
     "start_time": "2021-01-13T12:16:26.489720Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用户点击、浏览、加入购物车、购买行为统计\n",
    "\"\"\"\n",
    "# 1. 每个用户交互过的商店个数\n",
    "df_all_data_test = user_cnt(df_all_data_test, 'seller_path', 'user_cnt')\n",
    "# 2. 每个用户交互过的不同商店个数\n",
    "df_all_data_test = user_nunique(df_all_data_test, 'seller_path', 'seller_nunique')\n",
    "# 3. 每个用户交互过的不同商品品类个数\n",
    "df_all_data_test = user_nunique(df_all_data_test, 'cat_path', 'cat_nunique')\n",
    "# 4. 每个用户交互过的不同品牌个数\n",
    "df_all_data_test = user_nunique(df_all_data_test, 'brand_path', 'brand_nunique')\n",
    "# 5. 每个用户交互过的不同商品个数\n",
    "df_all_data_test = user_nunique(df_all_data_test, 'item_path', 'item_nunique')\n",
    "# 6. 每个用户活跃的天数\n",
    "df_all_data_test = user_nunique(df_all_data_test, 'time_stamp_path', 'time_stamp_nunique')\n",
    "# 7. 每个用户不同行为种类个数\n",
    "df_all_data_test = user_nunique(df_all_data_test, 'action_type_path', 'action_type_nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:27.778952Z",
     "start_time": "2021-01-13T12:16:27.737775Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:29.781138Z",
     "start_time": "2021-01-13T12:16:29.391233Z"
    }
   },
   "outputs": [],
   "source": [
    "# time_tamp:购买时间（格式：mmdd）\n",
    "# 8. 每个用户最近一次活跃时间 \n",
    "df_all_data_test = user_max(df_all_data_test, 'time_stamp_path', 'time_stamp_max')\n",
    "# 9. 每个用户第一次活跃时间\n",
    "df_all_data_test = user_min(df_all_data_test, 'time_stamp_path', 'time_stamp_min')\n",
    "# 10. 活跃时间方差\n",
    "df_all_data_test = user_std(df_all_data_test, 'time_stamp_path', 'time_stamp_std')\n",
    "# 11. 最早和最晚日期相隔天数\n",
    "df_all_data_test['time_stamp_range'] = df_all_data_test['time_stamp_max'] - df_all_data_test['time_stamp_min']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:31.117984Z",
     "start_time": "2021-01-13T12:16:30.741832Z"
    }
   },
   "outputs": [],
   "source": [
    "# 12. 用户最喜欢的商店(交互过最多的seller_id)\n",
    "df_all_data_test = user_most_n(df_all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 13. 用户最喜欢的品类(交互过最多的cat_id)\n",
    "df_all_data_test = user_most_n(df_all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 14. 用户最喜欢的品牌(交互过最多的brand_id)\n",
    "df_all_data_test = user_most_n(df_all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 15. 用户最喜欢的做的行为动作(最多的action_type)\n",
    "df_all_data_test = user_most_n(df_all_data_test, 'action_type_path', 'action_type_1', n=1)\n",
    "\n",
    "# 16. 用户最喜欢的商店交互次数(交互过最多的seller_id)\n",
    "df_all_data_test = user_most_n_cnt(df_all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 17. 用户最喜欢的品类交互次数(交互过最多的cat_id)\n",
    "df_all_data_test = user_most_n_cnt(df_all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 18. 用户最喜欢的品牌交互次数(交互过最多的brand_id)\n",
    "df_all_data_test = user_most_n_cnt(df_all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 19. 用户最喜欢的做的行为动作交互次数(最多的action_type)\n",
    "df_all_data_test = user_most_n_cnt(df_all_data_test, 'action_type_path', 'action_type_1_cnt', n=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T11:18:11.722926Z",
     "start_time": "2021-01-11T11:18:11.718468Z"
    }
   },
   "source": [
    "#### 用户特征\n",
    "- 统计用户交互次数\n",
    "- 用户交互次数在所有用户中的对比\n",
    "- 用户点击次数在所有用户中的对比\n",
    "- 用户加入购物车次数在所有用户加入购物车次数中的地位\n",
    "- 用户购买次数在所有用户购买次数中的地位\n",
    "- 用户收藏次数在所有用户收藏次数中的地位\n",
    "- 统计用户不同行为的习惯\n",
    "- 统计用户的点击、加入购物车、收藏的购买转化率\n",
    "- 用户交互的时间信息（按天、次数）\n",
    "- 用户的活跃程度的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:34.088648Z",
     "start_time": "2021-01-13T12:16:34.067147Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用户统计特征: 点击、加入购物车、购买、收藏夹\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "copy & deepcopy\n",
    "我们寻常意义的复制就是深复制，即将被复制对象完全再复制一遍作为独立的新个体单独存在。\n",
    "所以改变原有被复制对象不会对已经复制出来的新对象产生影响。\n",
    "—–而浅复制并不会产生一个独立的对象单独存在，他只是将原有的数据块打上一个新标签，\n",
    "所以当其中一个标签被改变的时候，数据块就会发生变化，另一个标签也会随之改变。\n",
    "\"\"\"\n",
    "# 各种行为的店铺数量统计\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    \"\"\"\n",
    "    统计函数定义\n",
    "    根据不同行为的业务函数提取不同特征, 对不同的行为进行分别统计\n",
    "    action_type包含{0, 1, 2, 3}，0表示单击，1表示添加到购物车，2表示购买，3表示添加到收藏夹\n",
    "    \n",
    "    @param df_data: DataFrame\n",
    "    @param columns_list: list, column name to stats\n",
    "    @param action_type: \n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)  # deepcopy\n",
    "        if action_type != None:\n",
    "            # 加上需要统计的目标col: action_type_path\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            # 将每一个用户col内的元素以list的形式存入字典 \n",
    "            # 结果类似于{'seller_path': ['3152', '3152', '3022', ...], 'action_type_path': ['3', '3', '0', ...]}\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])  # 统计每一行用户的行为个数总和\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    # 如果是指定的action_type, 就把对应的col内容添加到data_txt中\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "                # 如果不是的话, 就跳过, data_txt为空字符串\n",
    "            data_out.append(data_txt)\n",
    "        return len(data_out)\n",
    "    \n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    \"\"\"\n",
    "    统计函数调用\n",
    "    各种行为的店铺数量统计\n",
    "    action_type包含{0, 1, 2, 3}，0表示单击，1表示添加到购物车，2表示购买，3表示添加到收藏夹\n",
    "    \n",
    "    @param df_data: DataFrame\n",
    "    @param columns_list: \n",
    "    @param action_type: \n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "# 各种行为的不同店铺数量统计\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            # 加上需要统计的目标col: action_type_path\n",
    "            col_list += ['action_type_path']\n",
    "        for col in col_list:\n",
    "            # 将每一个用户col内的元素以list的形式存入字典 \n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        \n",
    "        # set用于去重\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    \"\"\"\n",
    "    统计函数调用\n",
    "    各种行为的不同店铺数量统计\n",
    "    action_type包含{0, 1, 2, 3}，0表示单击，1表示添加到购物车，2表示购买，3表示添加到收藏夹\n",
    "    \n",
    "    @param df_data: DataFrame\n",
    "    @param columns_list: \n",
    "    @param action_type: \n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis=1)\n",
    "\n",
    "    return df_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:35.469178Z",
     "start_time": "2021-01-13T12:16:34.845548Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "统计店铺被用户点击次数，加购次数，购买次数，收藏次数\n",
    "\"\"\"\n",
    "\n",
    "# (0: 表示单击) 用户对商店商品的点击次数\n",
    "df_all_data_test = user_col_cnt(df_all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# (1: 表示添加到购物车) 用户对商店商品的添加购物车次数\n",
    "df_all_data_test = user_col_cnt(df_all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# (2: 表示购买) 用户对商店的购买商品次数\n",
    "df_all_data_test = user_col_cnt(df_all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# (3: 表示添加到收藏夹) 用户对商店商品的添加购物车次数\n",
    "df_all_data_test = user_col_cnt(df_all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T10:48:22.691512Z",
     "start_time": "2021-01-12T10:48:22.685064Z"
    }
   },
   "source": [
    "#### 组合特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:36.779549Z",
     "start_time": "2021-01-13T12:16:36.298423Z"
    }
   },
   "outputs": [],
   "source": [
    "# 点击商店中物品的次数\n",
    "df_all_data_test = user_col_cnt(df_all_data_test,  ['seller_path', 'item_path'], '0', 'user_cnt_0')\n",
    "\n",
    "# 点击不同商店中不同物品的次数\n",
    "df_all_data_test = user_col_nunique(df_all_data_test,  ['seller_path', 'item_path'], '0', 'seller_nunique_0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**目前提取的特征**\n",
    "- label : 是否复购的标示\n",
    "- merchant_id : 商店id\n",
    "- prob : 给定客户是给定商家的重复购买者的概率，取值在[0, 1]\n",
    "- user_id : 用户id\n",
    "- age_range : 用户年龄范围(分类)\n",
    "- gender : 用户性别\n",
    "- item_path : 用户交互过的商品id集合\n",
    "- cat_path : 用户交互过的商品品类id集合\n",
    "- seller_path : 用户交互过的商店id集合\n",
    "- brand_path : 用户交互过的品牌id集合\n",
    "- time_stamp_path : 用户购买时间集合 (格式：mmdd)\n",
    "- action_type_path : 用户交互行为类型集合\n",
    "- user_cnt : 用户交互过的商店个数\n",
    "- seller_nunique : 用户交互过的不同商店个数\n",
    "- cat_nunique : 用户交互过的不同品类个数\n",
    "- brand_nunique : 用户交互过的不同品牌个数\n",
    "- item_nunique : 用户交互过的不同商品个数\n",
    "- time_stamp_nunique : 用户活跃的天数\n",
    "- action_type_nunique : 用户不同行为种类个数\n",
    "- time_stamp_max : 用户最近一次活跃ts (格式: MMDD)\n",
    "- time_stamp_min : 用户最早一次活跃ts (格式: MMDD)\n",
    "- time_stamp_std : 用户最早一次活跃ts标准差\n",
    "- time_stamp_range : 用户最早一次活跃与最近一次活跃日期差值 (非天数, 数值本身没有意义, 但是不同用户间可以比大小)\n",
    "- seller_most_1 : 用户交互最多次的商店\n",
    "- cat_most_1 : 用户交互最多次的商品品类\n",
    "- brand_most_1 : 用户交互最多次的品牌\n",
    "- action_type_1 : 用户最多次的交互行为\n",
    "- seller_most_1_cnt : 用户交互最多次的商店 交互次数\n",
    "- cat_most_1_cnt : 用户交互最多次的商品品类 交互次数\n",
    "- brand_most_1_cnt : 用户交互最多次的品牌 交互次数\n",
    "- action_type_1_cnt : 用户最多次的交互行为 次数\n",
    "- user_cnt_0 : 用户点击商店中物品的次数\n",
    "- user_cnt_1 : 用户对商店商品的添加购物车次数\n",
    "- user_cnt_2 : 用户对商店商品的购买次数\n",
    "- user_cnt_3 : 用户对商店商品的添加购物车次数\n",
    "- seller_nunique_0 : 点击不同商店中不同物品的个数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T10:58:17.817512Z",
     "start_time": "2021-01-13T10:58:17.813653Z"
    }
   },
   "source": [
    "# 利用countvector, tfidf提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T11:10:17.580518Z",
     "start_time": "2021-01-13T11:10:17.574711Z"
    }
   },
   "source": [
    "## 词袋模型TF-IDF\n",
    "[参考资料: sklearn-TfidfVectorizer彻底说清楚](https://zhuanlan.zhihu.com/p/67883024)\n",
    "<br>\n",
    "<br>\n",
    "- **TF （Term Frequency）—— “单词频率”**\n",
    "<br>\n",
    "1. 意思就是说，我们计算一个查询关键字中某一个单词在目标文档中出现的次数。举例说来，如果我们要查询 “Car Insurance”，那么对于每一个文档，我们都计算“Car” 这个单词在其中出现了多少次，“Insurance”这个单词在其中出现了多少次。这个就是 TF 的计算方法。\n",
    "<br>\n",
    "2. TF背后的隐含的假设是，查询关键字中的单词应该相对于其他单词更加重要，而文档的重要程度，也就是相关度，与单词在文档中出现的次数成正比。比如，“Car” 这个单词在文档 A 里出现了 5 次，而在文档 B 里出现了 20 次，那么 TF 计算就认为文档 B 可能更相关。\n",
    "<br>\n",
    "3. 然而，仅有 TF 不能比较完整地描述文档的相关度。因为语言的因素，有一些单词可能会比较自然地在很多文档中反复出现，比如英语中的 “The”、“An”、“But” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分。然而，如果我们要搜索 “How to Build A Car” 这个关键词，其中的 “How”、“To” 以及 “A” 都极可能在绝大多数的文档中出现，这个时候 TF 就无法帮助我们区分文档的相关度了。\n",
    "\n",
    "- **IDF（Inverse Document Frequency）—— “逆文档频率”**\n",
    "<br>\n",
    "1. 思路就是我们需要去 “惩罚”（Penalize）那些出现在太多文档中的单词。也就是说，真正携带 “相关” 信息的单词仅仅出现在相对比较少，有时候可能是极少数的文档里。\n",
    "<br>\n",
    "2. 这个信息，很容易用 “文档频率” 来计算，也就是，有多少文档涵盖了这个单词。很明显，如果有太多文档都涵盖了某个单词，这个单词也就越不重要，或者说是这个单词就越没有信息量。\n",
    "<br>\n",
    "3. 因此，我们需要对 TF 的值进行修正，而 IDF 的想法是用 DF 的倒数来进行修正。倒数的应用正好表达了这样的思想，DF 值越大越不重要。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:39.114814Z",
     "start_time": "2021-01-13T12:16:39.105793Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "文本表示模型: 利用countvector，tfidf提取特征\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python sklearn 中的TfidfVectorizer参数解析](https://blog.csdn.net/weixin_42462804/article/details/105433680)\n",
    "- stop_words: string, list, or None(default), 如果未english，用于英语内建的停用词列表，如果未list，该列表被假定为包含停用词，列表中的所有词都将从令牌中删除， 如果None，不使用停用词。\n",
    "- max_df: 可以被设置为范围[0.7, 1.0)的值，基于内部预料词频来自动检测和过滤停用词。\n",
    "- ngram_range (min,max): 是指将text分成min，min+1，min+2,.........max 个不同的词组。比如'Python is useful'中ngram_range(1,3)之后可得到'Python' 'is' 'useful' 'Python is' 'is useful' 和'Python is useful'如果是ngram_range(1,1)则只能得到单个单词'Python' 'is'和'useful'\n",
    "- analyzer: string，{'word', 'char'} or callable定义特征为词（word）或n-gram字符\n",
    "- max_df: float in range [0.0, 1.0] or int, optional, 1.0 by default当构建词汇表时，严格忽略高于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。\n",
    "- binary: boolean， False by default, 如果为True，所有非零计数被设置为1，这对于离散概率模型是有用的，建立二元事件模型，而不是整型计数。\n",
    "- token_pattern: 正则表达式显示了”token“的构成，仅当analyzer == ‘word’时才被使用。\n",
    "- sublinear_tf: boolean， optional应用线性缩放TF，例如，使用1+log(tf)覆盖tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:16:40.838225Z",
     "start_time": "2021-01-13T12:16:40.498831Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TF-IDF\n",
    "\"\"\"\n",
    "# 实例化tf实例\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,  # 英语停用词\n",
    "                           ngram_range=(1, 1),  # 指将text分成min，min+1，min+2,.........max 个不同的词组\n",
    "                           max_features=100)\n",
    "\n",
    "# 对用户交互过商店序列进行特征提取 ==> 用户交互过的商店序列向量化\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    df_all_data_test[col] = df_all_data_test[col].astype(str)\n",
    "    tfidfVec.fit(df_all_data_test[col])\n",
    "    # 训练, 构建词汇表以及词项idf值, 并将输入文本列表转成VSM矩阵形式\n",
    "    data_ = tfidfVec.transform(df_all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        # 数据拼接\n",
    "        data_cat = sparse.hstack((data_cat, data_))  # Stack sparse matrices horizontally (column wise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:17:52.512895Z",
     "start_time": "2021-01-13T12:17:52.506081Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "特征重命名以及特征合并\n",
    "\"\"\"\n",
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "df_all_data_test = pd.concat([df_all_data_test, df_tfidf], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T12:18:10.992791Z",
     "start_time": "2021-01-13T12:18:10.990269Z"
    }
   },
   "source": [
    "# 嵌入特征 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:08:47.694862Z",
     "start_time": "2021-01-14T06:08:45.515493Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[gensim.models.word2vec参数说明](https://blog.csdn.net/u011748542/article/details/85880852)\n",
    "- sentences (iterable of iterables, optional) – 供训练的句子，可以使用简单的列表，但是对于大语料库，建议直接从磁盘/网络流迭代传输句子。参阅word2vec模块中的BrownCorpus，Text8Corpus或LineSentence。\n",
    "- corpus_file (str, optional) – LineSentence格式的语料库文件路径。\n",
    "- size (int, optional) – word向量的维度。\n",
    "- window (int, optional) – 一个句子中当前单词和被预测单词的最大距离。\n",
    "- min_count (int, optional) – 忽略词频小于此值的单词。\n",
    "- workers (int, optional) – 训练模型时使用的线程数。\n",
    "- sg ({0, 1}, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW.\n",
    "- hs ({0, 1}, optional) – 1: 采用hierarchical softmax训练模型; 0: 使用负采样。\n",
    "- negative (int, optional) – > 0: 使用负采样，设置多个负采样(通常在5-20之间)。\n",
    "- ns_exponent (float, optional) – 负采样分布指数。1.0样本值与频率成正比，0.0样本所有单词均等，负值更多地采样低频词。\n",
    "- cbow_mean ({0, 1}, optional) – 0: 使用上下文单词向量的总和; 1: 使用均值，适用于使用CBOW。\n",
    "- alpha (float, optional) – 初始学习率。\n",
    "- min_alpha (float, optional) – 随着训练的进行，学习率线性下降到min_alpha。\n",
    "- seed (int, optional) – 随机数发生器种子。\n",
    "- max_vocab_size (int, optional) – 词汇构建期间RAM的限制; 如果有更多的独特单词，则修剪不常见的单词。 每1000万个类型的字需要大约1GB的RAM。\n",
    "- max_final_vocab (int, optional) – 自动选择匹配的min_count将词汇限制为目标词汇大小。\n",
    "- sample (float, optional) – 高频词随机下采样的配置阈值，范围是(0,1e-5)。\n",
    "- hashfxn (function, optional) – 哈希函数用于随机初始化权重，以提高训练的可重复性。\n",
    "- iter (int, optional) – 迭代次数。\n",
    "- trim_rule (function, optional) – 词汇修剪规则，指定某些词语是否应保留在词汇表中，修剪掉或使用默认值处理。\n",
    "- sorted_vocab ({0, 1}, optional) – 如果为1，则在分配单词索引前按降序对词汇表进行排序。\n",
    "- batch_words (int, optional) – 每一个batch传递给线程单词的数量。\n",
    "- compute_loss (bool, optional) – 如果为True，则计算并存储可使用get_latest_training_loss()检索的损失值。\n",
    "- callbacks (iterable of CallbackAny2Vec, optional) – 在训练中特定阶段执行回调序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T02:39:18.278644Z",
     "start_time": "2021-01-16T02:39:16.252781Z"
    }
   },
   "outputs": [],
   "source": [
    "# train w2v model\n",
    "\"\"\"\n",
    "size=100: word向量的维度为100\n",
    "window=5: 一个句子中当前单词和被预测单词的最大距离为5\n",
    "min_count=5: 忽略词频小于5的单词\n",
    "\"\"\"\n",
    "model = gensim.models.Word2Vec(df_all_data_test['seller_path'].apply(lambda x: x.split(' ')),\n",
    "                               size=100, window=5, min_count=5, workers=4)\n",
    "# save model to local\n",
    "# model.save(tmp_res_path + 'product2vec.model')\n",
    "# model = gensim.models.Word2Vec.load(tmp_res_path + \"seller2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T05:14:12.077004Z",
     "start_time": "2021-01-19T05:14:12.069276Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_w2v_(x, model, size=100):\n",
    "    \"\"\"\n",
    "    \n",
    "    @param x: \n",
    "    @type x:\n",
    "    @param model:\n",
    "    @type model:\n",
    "    @param size:\n",
    "    @type size: int\n",
    "    @return:\n",
    "    @rtype:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            # word: each seller in col 'seller_path'\n",
    "            if word in model.wv.vocab:\n",
    "                # if the seller in model\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    # if the first seller, initial\n",
    "                    vec = np.zeros(size)\n",
    "                # sum the vector\n",
    "                vec += model.wv[word]\n",
    "        # calculate the sum of vectors\n",
    "        return vec / i\n",
    "    \n",
    "    except:\n",
    "        return np.zeros(size)\n",
    "\n",
    "    \n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    \"\"\"\n",
    "    \n",
    "    @param df_data: \n",
    "    @type df_data: DataFrame\n",
    "    @param columns: column name\n",
    "    @type columns: string\n",
    "    @param model:\n",
    "    @type model: \n",
    "    @param size: size of embedding vector\n",
    "    @type size: int\n",
    "    @return: \n",
    "    @rtype: DataFrame\n",
    "    \"\"\"\n",
    "    data_array = []\n",
    "    \n",
    "    for index, row in df_data.iterrows():\n",
    "        # for each row idx and content\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    \n",
    "    return pd.DataFrame(data_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T05:18:02.718340Z",
     "start_time": "2021-01-19T05:18:02.668347Z"
    }
   },
   "outputs": [],
   "source": [
    "df_embedding = get_mean_w2v(df_all_data_test, 'seller_path', model, 100)\n",
    "# rename emdding column name\n",
    "df_embedding.columns = ['embedding_' + str(i) for i in df_embedding.columns]\n",
    "\n",
    "df_all_data_test = pd.concat([df_all_data_test, df_embedding], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking分类特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T05:19:02.345835Z",
     "start_time": "2021-01-19T05:19:02.341931Z"
    }
   },
   "source": [
    "## stacking package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T05:40:06.872008Z",
     "start_time": "2021-01-19T05:40:06.722247Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss, mean_absolute_error, mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}